{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "import segmentation_models as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p_dir = 'data_p/l/'\n",
    "image_dir = data_p_dir + 'image/'\n",
    "label_dir = data_p_dir + 'label/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file_paths = tf.data.Dataset.list_files(image_dir + '*.jpg', shuffle=False)\n",
    "label_file_paths = tf.data.Dataset.list_files(label_dir + '*.jpg', shuffle=False)\n",
    "\n",
    "dataset = tf.data.Dataset.zip((image_file_paths, label_file_paths))\n",
    "\n",
    "# for i, l in dataset.take(3):\n",
    "#     print(i, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = tf.cast(img, tf.float32) / 255.\n",
    "    return img\n",
    "\n",
    "def process_label(path):\n",
    "    mask = tf.io.read_file(path)\n",
    "    img = tf.io.parse_tensor(mask, out_type=tf.float32)\n",
    "    return img\n",
    "\n",
    "def process_batch(image, label):\n",
    "    X = process_img(image)\n",
    "    y = process_label(label)\n",
    "    return X, y\n",
    "\n",
    "dataset = dataset.map(process_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ds_element_overlay(background, overlay):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.imshow(background)\n",
    "    ax.imshow(overlay, alpha=0.3)\n",
    "\n",
    "def plot_ds_element(background, overlay):\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    axs[0].imshow(background)\n",
    "    axs[1].imshow(overlay)\n",
    "    plt.show()\n",
    "\n",
    "# for i, m in dataset.skip(9).take(1):\n",
    "#     plot_ds_element(i, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(buffer_size=10000, seed=SEED)\n",
    "\n",
    "# Define the split ratio (e.g., 80% for training, 20% for validation)\n",
    "split_ratio = 0.8\n",
    "num_samples = dataset.cardinality().numpy()\n",
    "\n",
    "num_train = int(split_ratio * num_samples)\n",
    "num_val = num_samples - num_train\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_ds = dataset.take(num_train)\n",
    "val_ds = dataset.skip(num_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.Unet('efficientnetb2', classes=1, activation='sigmoid')\n",
    "\n",
    "metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5)]\n",
    "model.compile('adam', sm.losses.DiceLoss(), metrics)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callbacks():\n",
    "    from datetime import datetime\n",
    "    logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "    checkpoint_path = \"model_checkpoint.h5\"\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return [tensorboard_callback, checkpoint_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(4).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(4).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "model.fit(train_ds, epochs=100, validation_data=val_ds, callbacks=callbacks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP = 80\n",
    "d = tf.data.Dataset.zip((image_file_paths, label_file_paths)).map(process_batch)\n",
    "\n",
    "item = d.batch(1).skip(SKIP).take(1)\n",
    "print(item)\n",
    "\n",
    "for i, l in item:\n",
    "    plot_ds_element(i[0], l[0])\n",
    "\n",
    "for i, l in item:\n",
    "    p = model.predict(i)\n",
    "    plot_ds_element(p[0], p[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
